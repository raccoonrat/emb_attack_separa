{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vcw30NIoTCYz"
      },
      "source": [
        "# OKR (Opportunistic Keyed Routing) 水印实验\n",
        "\n",
        "本notebook实现了OKR水印算法，可以在Google Colab上运行实验。\n",
        "\n",
        "## 核心思想\n",
        "\n",
        "OKR是一种\"机会主义\"水印方法：\n",
        "- 只在\"安全区域\"（indifference zone）内修改路由决策\n",
        "- 安全区域定义：`max_logit - current_logit < epsilon`\n",
        "- 使用LSH投影计算水印偏好\n",
        "- 如果不在安全区，保持原始路由（fail-open）\n",
        "\n",
        "## 实验流程\n",
        "\n",
        "1. 安装依赖\n",
        "2. 加载模型和分词器\n",
        "3. 注入OKR水印\n",
        "4. 生成带水印的文本\n",
        "5. 检测水印\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozkNd-BXTCY0"
      },
      "source": [
        "## 1. 安装依赖\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e2TLaLG1TCY0"
      },
      "outputs": [],
      "source": [
        "%pip install -q torch transformers accelerate sentencepiece tqdm numpy matplotlib\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mKkWswpsTCY0"
      },
      "source": [
        "## 2. 导入库和核心代码\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JMqvjihnTCY0"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from typing import Union, Optional, Tuple\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "from pathlib import Path\n",
        "import hashlib\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6Pjxe3hTCY0"
      },
      "source": [
        "### 2.1 OKR核心路由逻辑 (okr_kernel.py)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bnF620VmTCY0"
      },
      "outputs": [],
      "source": [
        "class OKRRouter(nn.Module):\n",
        "    \"\"\"\n",
        "    Opportunistic Keyed Routing 路由器\n",
        "\n",
        "    核心思想：只在安全区域内修改路由，保持质量\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 input_dim: int,\n",
        "                 num_experts: int,\n",
        "                 top_k: int = 2,\n",
        "                 epsilon: float = 1.5):\n",
        "        super().__init__()\n",
        "        self.top_k = top_k\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "        # 标准的 Gating 层\n",
        "        self.gate_network = nn.Linear(input_dim, num_experts, bias=False)\n",
        "\n",
        "        # 水印私钥投影矩阵\n",
        "        self.register_buffer(\n",
        "            \"secret_projection\",\n",
        "            torch.randn(input_dim, num_experts)\n",
        "        )\n",
        "\n",
        "    def forward(self, hidden_states: torch.Tensor):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            hidden_states: [batch_size, seq_len, input_dim]\n",
        "        Returns:\n",
        "            routing_weights, selected_experts\n",
        "        \"\"\"\n",
        "        # 1. 计算原始路由分数\n",
        "        raw_logits = self.gate_network(hidden_states)\n",
        "\n",
        "        # 2. 计算水印偏好 (LSH投影)\n",
        "        if self.secret_projection.device != hidden_states.device:\n",
        "            self.secret_projection = self.secret_projection.to(hidden_states.device)\n",
        "        watermark_bias = torch.matmul(hidden_states, self.secret_projection)\n",
        "\n",
        "        # 3. 计算安全掩码 (Indifference Zone)\n",
        "        max_logits, _ = raw_logits.max(dim=-1, keepdim=True)\n",
        "        safe_mask = raw_logits >= (max_logits - self.epsilon)\n",
        "\n",
        "        # 4. 机会主义注入\n",
        "        if watermark_bias.device != raw_logits.device:\n",
        "            watermark_bias = watermark_bias.to(raw_logits.device)\n",
        "\n",
        "        modified_scores = torch.where(\n",
        "            safe_mask,\n",
        "            watermark_bias,\n",
        "            torch.tensor(-1e9, device=raw_logits.device, dtype=raw_logits.dtype)\n",
        "        )\n",
        "\n",
        "        # 5. 选取 Top-K\n",
        "        _, selected_experts = torch.topk(modified_scores, self.top_k, dim=-1)\n",
        "\n",
        "        # 6. 计算权重 (使用原始logits)\n",
        "        router_logits = torch.gather(raw_logits, -1, selected_experts)\n",
        "        routing_weights = F.softmax(router_logits, dim=-1)\n",
        "\n",
        "        return routing_weights, selected_experts\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzQ3x6XzTCY1"
      },
      "source": [
        "### 2.2 OKR注入代码 (okr_patch.py)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YwL_UEp8TCY1"
      },
      "outputs": [],
      "source": [
        "def _okr_forward_core(router: OKRRouter, hidden_states: torch.Tensor):\n",
        "    \"\"\"OKR核心前向传播逻辑\"\"\"\n",
        "    if hidden_states.dtype != router.gate_network.weight.dtype:\n",
        "        hidden_states = hidden_states.to(dtype=router.gate_network.weight.dtype)\n",
        "\n",
        "    raw_logits = router.gate_network(hidden_states)\n",
        "\n",
        "    if router.secret_projection.device != hidden_states.device:\n",
        "        router.secret_projection = router.secret_projection.to(hidden_states.device)\n",
        "\n",
        "    watermark_bias = torch.matmul(hidden_states, router.secret_projection)\n",
        "\n",
        "    max_logits, _ = raw_logits.max(dim=-1, keepdim=True)\n",
        "    safe_mask = raw_logits >= (max_logits - router.epsilon)\n",
        "\n",
        "    if watermark_bias.device != raw_logits.device:\n",
        "        watermark_bias = watermark_bias.to(raw_logits.device)\n",
        "\n",
        "    modified_scores = torch.where(\n",
        "        safe_mask,\n",
        "        watermark_bias,\n",
        "        torch.tensor(-1e9, device=watermark_bias.device, dtype=watermark_bias.dtype)\n",
        "    )\n",
        "\n",
        "    _, selected_experts = torch.topk(modified_scores, router.top_k, dim=-1)\n",
        "\n",
        "    router_logits = torch.gather(raw_logits, -1, selected_experts)\n",
        "    routing_weights = F.softmax(router_logits, dim=-1)\n",
        "\n",
        "    return routing_weights, selected_experts\n",
        "\n",
        "\n",
        "def inject_okr(model: AutoModelForSeq2SeqLM,\n",
        "               epsilon: float = 1.5,\n",
        "               secret_key: Optional[str] = None) -> AutoModelForSeq2SeqLM:\n",
        "    \"\"\"\n",
        "    将OKR注入到MoE模型中\n",
        "\n",
        "    Args:\n",
        "        model: 预训练的MoE模型\n",
        "        epsilon: 质量容忍阈值\n",
        "        secret_key: 可选的密钥（用于初始化secret_projection）\n",
        "\n",
        "    Returns:\n",
        "        patched_model: 已注入OKR的模型\n",
        "    \"\"\"\n",
        "    count = 0\n",
        "\n",
        "    # 获取模型配置\n",
        "    if hasattr(model.config, 'num_local_experts'):\n",
        "        num_experts = model.config.num_local_experts\n",
        "        top_k = getattr(model.config, 'num_experts_per_tok', 1)\n",
        "    elif hasattr(model.config, 'num_experts'):\n",
        "        num_experts = model.config.num_experts\n",
        "        top_k = getattr(model.config, 'num_experts_per_tok', 1)\n",
        "    else:\n",
        "        raise ValueError(\"无法检测模型配置：找不到num_experts或num_local_experts\")\n",
        "\n",
        "    print(f\"DEBUG: Initial model config: num_experts={num_experts}, top_k={top_k}\")\n",
        "\n",
        "    # 查找decoder\n",
        "    decoder = None\n",
        "    if hasattr(model, 'decoder'):\n",
        "        decoder = model.decoder\n",
        "        print(f\"DEBUG: Found decoder directly in model: {type(decoder)}\")\n",
        "    elif hasattr(model, 'model') and hasattr(model.model, 'decoder'):\n",
        "        decoder = model.model.decoder\n",
        "        print(f\"DEBUG: Found decoder in model.model: {type(decoder)}\")\n",
        "\n",
        "    if decoder is None:\n",
        "        print(\"DEBUG: Could not find decoder.\")\n",
        "        raise ValueError(\"无法找到decoder\")\n",
        "\n",
        "    # 获取decoder blocks\n",
        "    decoder_blocks = None\n",
        "    if hasattr(decoder, 'block'):\n",
        "        decoder_blocks = decoder.block\n",
        "        print(f\"DEBUG: Found decoder blocks as 'block': {type(decoder_blocks)}\")\n",
        "    elif hasattr(decoder, 'layers'):\n",
        "        decoder_blocks = decoder.layers\n",
        "        print(f\"DEBUG: Found decoder blocks as 'layers': {type(decoder_blocks)}\")\n",
        "\n",
        "    if decoder_blocks is None:\n",
        "        print(\"DEBUG: Could not find decoder blocks.\")\n",
        "        raise ValueError(\"无法找到decoder blocks\")\n",
        "\n",
        "    # 遍历所有层，查找MoE router\n",
        "    for layer_idx, layer in enumerate(decoder_blocks):\n",
        "        router = None\n",
        "\n",
        "        print(f\"DEBUG: Processing decoder layer {layer_idx}\")\n",
        "        print(f\"DEBUG: Layer type: {type(layer)}\")\n",
        "\n",
        "        # 查找router（Switch Transformer标准结构，T5 decoder block通常是 self-attn(0), cross-attn(1), FFN(2)）\n",
        "        if hasattr(layer, 'layer'):\n",
        "            layer_list = layer.layer\n",
        "            print(f\"DEBUG: Layer.layer type: {type(layer_list)}, length: {len(layer_list)}\")\n",
        "            if len(layer_list) > 2: # 确保有足够的层，索引2有效\n",
        "                ffn_layer = layer_list[2]\n",
        "                print(f\"DEBUG: FFN Layer type (index 2): {type(ffn_layer)}\")\n",
        "                # 修正: 路由器位于 ffn_layer.mlp.router\n",
        "                if hasattr(ffn_layer, 'mlp') and hasattr(ffn_layer.mlp, 'router'):\n",
        "                    router = ffn_layer.mlp.router\n",
        "                    print(f\"DEBUG: Found router in layer {layer_idx} at ffn_layer.mlp.router, type: {type(router)}\")\n",
        "                else:\n",
        "                    print(f\"DEBUG: FFN layer (index 2) in layer {layer_idx} does NOT have 'router' attribute directly or in 'mlp'.\")\n",
        "            else:\n",
        "                print(f\"DEBUG: layer.layer in layer {layer_idx} has {len(layer_list)} items, which is not enough for FFN at index 2.\")\n",
        "        else:\n",
        "            print(f\"DEBUG: Layer {layer_idx} does NOT have 'layer' attribute.\")\n",
        "\n",
        "\n",
        "        if router is None:\n",
        "            print(f\"DEBUG: No router found for layer {layer_idx}. Continuing...\")\n",
        "            continue\n",
        "\n",
        "        # 获取gate层和维度\n",
        "        gate_layer = None\n",
        "        input_dim = None\n",
        "        num_experts_found = None\n",
        "        router_dtype = torch.float32\n",
        "\n",
        "        if hasattr(router, 'classifier') and isinstance(router.classifier, torch.nn.Linear):\n",
        "            gate_layer = router.classifier\n",
        "            input_dim = gate_layer.in_features\n",
        "            num_experts_found = gate_layer.out_features\n",
        "            router_dtype = gate_layer.weight.dtype\n",
        "        elif hasattr(router, 'gate') and isinstance(router.gate, torch.nn.Linear):\n",
        "            gate_layer = router.gate\n",
        "            input_dim = gate_layer.in_features\n",
        "            num_experts_found = gate_layer.out_features\n",
        "            router_dtype = gate_layer.weight.dtype\n",
        "\n",
        "        if input_dim is None:\n",
        "            if hasattr(model.config, 'd_model'):\n",
        "                input_dim = model.config.d_model\n",
        "            else:\n",
        "                continue\n",
        "\n",
        "        if num_experts_found is None:\n",
        "            num_experts_found = num_experts\n",
        "\n",
        "        # 获取设备\n",
        "        device = next(router.parameters()).device\n",
        "\n",
        "        # 创建OKR router\n",
        "        okr_router = OKRRouter(input_dim, num_experts_found, top_k=top_k, epsilon=epsilon)\n",
        "        okr_router = okr_router.to(device=device, dtype=router_dtype)\n",
        "\n",
        "        # 复制原始权重\n",
        "        if gate_layer is not None:\n",
        "            with torch.no_grad():\n",
        "                gate_weight = gate_layer.weight\n",
        "                okr_weight = okr_router.gate_network.weight\n",
        "\n",
        "                if gate_weight.shape == okr_weight.shape:\n",
        "                    if gate_weight.dtype != okr_weight.dtype:\n",
        "                        okr_weight.data = gate_weight.data.to(dtype=okr_weight.dtype)\n",
        "                    else:\n",
        "                        okr_weight.copy_(gate_weight)\n",
        "                elif gate_weight.shape == okr_weight.shape[::-1]:\n",
        "                    weight_t = gate_weight.T\n",
        "                    if weight_t.dtype != okr_weight.dtype:\n",
        "                        okr_weight.data = weight_t.data.to(dtype=okr_weight.dtype)\n",
        "                    else:\n",
        "                        okr_weight.copy_(weight_t)\n",
        "\n",
        "        # 初始化secret_projection\n",
        "        if secret_key is not None:\n",
        "            seed = int(hashlib.sha256(secret_key.encode()).hexdigest()[:16], 16)\n",
        "            generator = torch.Generator(device=device)\n",
        "            generator.manual_seed(seed)\n",
        "            with torch.no_grad():\n",
        "                okr_router.secret_projection.data = torch.randn(\n",
        "                    input_dim, num_experts_found,\n",
        "                    generator=generator,\n",
        "                    device=device,\n",
        "                    dtype=router_dtype\n",
        "                )\n",
        "\n",
        "        # 保存OKR router\n",
        "        router._okr_router = okr_router\n",
        "\n",
        "        # 创建新的forward方法\n",
        "        original_forward = router.forward\n",
        "        is_first_router = (count == 0)\n",
        "\n",
        "        def make_okr_forward(orig_fwd, okr_rt, mdl, layer_id, is_first=False):\n",
        "            def okr_forward(hidden_states: torch.Tensor):\n",
        "                # 调用OKR核心逻辑\n",
        "                routing_weights, selected_experts = _okr_forward_core(okr_rt, hidden_states)\n",
        "\n",
        "                # 保存路由数据（仅第一个router）\n",
        "                if is_first:\n",
        "                    if not hasattr(router, '_okr_all_selected_experts'):\n",
        "                        router._okr_all_selected_experts = []\n",
        "\n",
        "                    batch_size, seq_len, _ = hidden_states.shape\n",
        "                    if seq_len == 1:\n",
        "                        router._okr_all_selected_experts.append(selected_experts)\n",
        "                    else:\n",
        "                        for i in range(seq_len):\n",
        "                            router._okr_all_selected_experts.append(selected_experts[:, i:i+1, :])\n",
        "\n",
        "                router._selected_experts = selected_experts\n",
        "\n",
        "                # 构造Switch Transformers格式的返回值\n",
        "                batch_size, seq_len, _ = hidden_states.shape\n",
        "                num_experts = okr_rt.gate_network.out_features\n",
        "\n",
        "                router_mask = torch.zeros(\n",
        "                    (batch_size, seq_len, num_experts),\n",
        "                    device=hidden_states.device,\n",
        "                    dtype=hidden_states.dtype\n",
        "                )\n",
        "                router_mask.scatter_(dim=-1, index=selected_experts, src=routing_weights)\n",
        "\n",
        "                router_probs = routing_weights.sum(dim=-1, keepdim=True)\n",
        "\n",
        "                router_logits = torch.full(\n",
        "                    (batch_size, seq_len, num_experts),\n",
        "                    float('-inf'),\n",
        "                    device=hidden_states.device,\n",
        "                    dtype=hidden_states.dtype\n",
        "                )\n",
        "                logits_values = torch.log(routing_weights + 1e-9)\n",
        "                router_logits.scatter_(dim=-1, index=selected_experts, src=logits_values)\n",
        "\n",
        "                return (router_mask, router_probs, router_logits)\n",
        "            return okr_forward\n",
        "\n",
        "        router.forward = make_okr_forward(original_forward, okr_router, model, layer_idx, is_first=is_first_router)\n",
        "        count += 1\n",
        "\n",
        "    if count == 0:\n",
        "        raise ValueError(\"未找到任何router层\")\n",
        "\n",
        "    # 注入clear_okr_stats方法\n",
        "    def clear_okr_stats(mdl):\n",
        "        if hasattr(mdl, '_okr_routing_data'):\n",
        "            mdl._okr_routing_data = {}\n",
        "        for name, module in mdl.named_modules():\n",
        "            if hasattr(module, '_okr_all_selected_experts'):\n",
        "                module._okr_all_selected_experts = []\n",
        "            if hasattr(module, '_selected_experts'):\n",
        "                module._selected_experts = None\n",
        "\n",
        "    import types\n",
        "    model.clear_okr_stats = types.MethodType(clear_okr_stats, model)\n",
        "\n",
        "    print(f\"✓ 已注入OKR到 {count} 个decoder层\")\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "onqmL6FUTCY1"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A_1vwcYfTCY1"
      },
      "outputs": [],
      "source": [
        "class OKRDetector:\n",
        "    \"\"\"\n",
        "    OKR水印检测器\n",
        "\n",
        "    核心验证逻辑：\n",
        "    1. 重算原本的Logits (Ground Truth)\n",
        "    2. 重算水印信号 (Expected Signal)\n",
        "    3. 重算机会窗口 (Opportunities)\n",
        "    4. 验证命中 (Check Hits)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model, epsilon: float = 1.5):\n",
        "        self.model = model\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    def detect(self, input_ids: torch.Tensor, attention_mask: Optional[torch.Tensor] = None,\n",
        "               decoder_input_ids: Optional[torch.Tensor] = None):\n",
        "        \"\"\"\n",
        "        对一段文本进行检测\n",
        "\n",
        "        Args:\n",
        "            input_ids: 输入token IDs [batch, seq_len] (encoder输入)\n",
        "            attention_mask: 注意力掩码 [batch, seq_len]\n",
        "            decoder_input_ids: decoder输入token IDs [batch, seq_len]\n",
        "\n",
        "        Returns:\n",
        "            score: 水印命中率 (0-1)\n",
        "            verdict: \"Watermarked\" 或 \"Clean\"\n",
        "        \"\"\"\n",
        "        # 获取路由数据\n",
        "        actual_selected_experts = self._extract_selected_experts()\n",
        "\n",
        "        if actual_selected_experts is None:\n",
        "            return 0.0, \"No routing data available\"\n",
        "\n",
        "        # 重新运行模型获取hidden_states\n",
        "        if decoder_input_ids is None:\n",
        "            decoder_input_ids = input_ids\n",
        "\n",
        "        # 保存并清空路由数据\n",
        "        router_for_detection = self._get_router()\n",
        "        saved_routing_data = {}\n",
        "        if router_for_detection:\n",
        "            if hasattr(router_for_detection, '_okr_all_selected_experts'):\n",
        "                saved_routing_data['_okr_all_selected_experts'] = router_for_detection._okr_all_selected_experts.copy() if router_for_detection._okr_all_selected_experts else []\n",
        "\n",
        "        if router_for_detection:\n",
        "            if hasattr(router_for_detection, '_okr_all_selected_experts'):\n",
        "                router_for_detection._okr_all_selected_experts = []\n",
        "\n",
        "        # 对齐长度\n",
        "        routing_seq_len = actual_selected_experts.shape[1]\n",
        "        if decoder_input_ids.shape[1] < routing_seq_len:\n",
        "            actual_selected_experts = actual_selected_experts[:, -decoder_input_ids.shape[1]:, :]\n",
        "        elif decoder_input_ids.shape[1] > routing_seq_len:\n",
        "            diff = decoder_input_ids.shape[1] - routing_seq_len\n",
        "            if diff == 1:\n",
        "                decoder_input_ids = decoder_input_ids[:, 1:]\n",
        "            else:\n",
        "                decoder_input_ids = decoder_input_ids[:, :routing_seq_len]\n",
        "\n",
        "        # 重新运行模型获取hidden_states\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                decoder_input_ids=decoder_input_ids,\n",
        "                output_hidden_states=True\n",
        "            )\n",
        "\n",
        "        # 恢复路由数据\n",
        "        if router_for_detection:\n",
        "            if hasattr(router_for_detection, '_okr_all_selected_experts'):\n",
        "                router_for_detection._okr_all_selected_experts = saved_routing_data.get('_okr_all_selected_experts', [])\n",
        "\n",
        "        # 获取decoder的最后一层hidden_states\n",
        "        if hasattr(outputs, 'decoder_hidden_states') and outputs.decoder_hidden_states:\n",
        "            hidden_states = outputs.decoder_hidden_states[-1]\n",
        "        elif hasattr(outputs, 'hidden_states') and outputs.hidden_states:\n",
        "            hidden_states = outputs.hidden_states[-1]\n",
        "        else:\n",
        "            return 0.0, \"No hidden states available\"\n",
        "\n",
        "        # 对齐长度\n",
        "        min_seq_len = min(hidden_states.shape[1], actual_selected_experts.shape[1])\n",
        "        hidden_states = hidden_states[:, :min_seq_len, :]\n",
        "        actual_selected_experts = actual_selected_experts[:, :min_seq_len, :]\n",
        "\n",
        "        return self.verify_batch(hidden_states, actual_selected_experts)\n",
        "\n",
        "    def verify_batch(self, hidden_states: torch.Tensor, actual_selected_experts: torch.Tensor) -> Tuple[float, str]:\n",
        "        \"\"\"\n",
        "        核心验证逻辑\n",
        "\n",
        "        Args:\n",
        "            hidden_states: [batch, seq, dim]\n",
        "            actual_selected_experts: [batch, seq, top_k]\n",
        "\n",
        "        Returns:\n",
        "            score: 水印命中率 (0-1)\n",
        "            verdict: \"Watermarked\" 或 \"Clean\"\n",
        "        \"\"\"\n",
        "        router = self._get_router()\n",
        "        if router is None:\n",
        "            return 0.0, \"No router found\"\n",
        "\n",
        "        okr_router = None\n",
        "        if hasattr(router, '_okr_router'):\n",
        "            okr_router = router._okr_router\n",
        "        elif hasattr(router, 'gate_network') and hasattr(router, 'secret_projection'):\n",
        "            okr_router = router\n",
        "\n",
        "        if okr_router is None:\n",
        "            return 0.0, \"No OKR router found\"\n",
        "\n",
        "        # 1. 重算原本的Logits\n",
        "        raw_logits = okr_router.gate_network(hidden_states)\n",
        "        max_logits, _ = raw_logits.max(dim=-1, keepdim=True)\n",
        "\n",
        "        # 2. 重算水印信号\n",
        "        if okr_router.secret_projection.device != hidden_states.device:\n",
        "            secret_proj = okr_router.secret_projection.to(hidden_states.device)\n",
        "        else:\n",
        "            secret_proj = okr_router.secret_projection\n",
        "\n",
        "        watermark_bias = torch.matmul(hidden_states, secret_proj)\n",
        "\n",
        "        # 3. 重算机会窗口\n",
        "        safe_mask = raw_logits >= (max_logits - self.epsilon)\n",
        "        num_safe_experts = safe_mask.sum(dim=-1)\n",
        "        valid_opportunity_mask = (num_safe_experts >= 2)\n",
        "\n",
        "        if valid_opportunity_mask.sum() == 0:\n",
        "            return 0.0, \"No Opportunities\"\n",
        "\n",
        "        # 4. 验证命中\n",
        "        masked_watermark_scores = torch.where(\n",
        "            safe_mask,\n",
        "            watermark_bias,\n",
        "            torch.tensor(-1e9, device=watermark_bias.device, dtype=watermark_bias.dtype)\n",
        "        )\n",
        "        target_expert = torch.argmax(masked_watermark_scores, dim=-1)\n",
        "\n",
        "        target_expert_expanded = target_expert.unsqueeze(-1)\n",
        "        hits = (actual_selected_experts == target_expert_expanded).any(dim=-1)\n",
        "\n",
        "        # 5. 统计分数\n",
        "        valid_hits = hits[valid_opportunity_mask]\n",
        "        score = valid_hits.float().mean().item()\n",
        "\n",
        "        # 6. 判断阈值\n",
        "        num_experts = raw_logits.shape[-1]\n",
        "        random_baseline = 1.0 / num_experts\n",
        "        threshold = random_baseline * 2.0\n",
        "\n",
        "        verdict = \"Watermarked\" if score > threshold else \"Clean\"\n",
        "        return score, verdict\n",
        "\n",
        "    def _get_router(self):\n",
        "        \"\"\"从模型中提取路由器\"\"\"\n",
        "        for name, module in self.model.named_modules():\n",
        "            if hasattr(module, '_okr_router'):\n",
        "                return module\n",
        "            if hasattr(module, 'secret_projection') and hasattr(module, 'gate_network'):\n",
        "                return module\n",
        "        return None\n",
        "\n",
        "    def _extract_selected_experts(self) -> Optional[torch.Tensor]:\n",
        "        \"\"\"从模型中提取实际选择的专家\"\"\"\n",
        "        router_for_detection = self._get_router()\n",
        "        if router_for_detection and hasattr(router_for_detection, '_okr_all_selected_experts'):\n",
        "            all_experts = router_for_detection._okr_all_selected_experts\n",
        "            if all_experts and len(all_experts) > 0:\n",
        "                concatenated = torch.cat(all_experts, dim=1)\n",
        "                return concatenated\n",
        "\n",
        "        router_found = False\n",
        "        for name, module in self.model.named_modules():\n",
        "            if hasattr(module, '_okr_all_selected_experts'):\n",
        "                all_experts = module._okr_all_selected_experts\n",
        "                if all_experts and len(all_experts) > 0:\n",
        "                    if not router_found:\n",
        "                        concatenated = torch.cat(all_experts, dim=1)\n",
        "                        router_found = True\n",
        "                        return concatenated\n",
        "\n",
        "        return None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Du_BFK2TCY1"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LicnwyeJTCY1"
      },
      "outputs": [],
      "source": [
        "# 实验配置\n",
        "CONFIG = {\n",
        "    \"model_name\": \"google/switch-base-8\",  # Switch Transformers模型\n",
        "    \"epsilon\": 1.5,  # 质量容忍阈值\n",
        "    \"secret_key\": \"OKR_COLAB_EXPERIMENT_KEY\",  # 水印密钥\n",
        "    \"num_samples\": 10,  # 实验样本数（Colab上建议用小样本）\n",
        "    \"max_length\": 128,  # 最大生成长度\n",
        "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "}\n",
        "\n",
        "print(f\"设备: {CONFIG['device']}\")\n",
        "print(f\"模型: {CONFIG['model_name']}\")\n",
        "print(f\"Epsilon: {CONFIG['epsilon']}\")\n",
        "print(f\"样本数: {CONFIG['num_samples']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3tbv5OnNTCY1"
      },
      "source": [
        "## 4. 加载模型和分词器\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xtHBF1hJTCY1"
      },
      "outputs": [],
      "source": [
        "print(\"加载分词器...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(CONFIG[\"model_name\"])\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "print(\"加载模型...\")\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
        "    CONFIG[\"model_name\"],\n",
        "    torch_dtype=torch.float32,\n",
        "    device_map=\"auto\" if CONFIG[\"device\"] == \"cuda\" else None,\n",
        "    low_cpu_mem_usage=True\n",
        ")\n",
        "model.eval()\n",
        "\n",
        "if CONFIG[\"device\"] == \"cpu\":\n",
        "    model = model.to(CONFIG[\"device\"])\n",
        "\n",
        "print(\"✓ 模型加载完成\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DltIyUrLTCY2"
      },
      "source": [
        "## 5. 注入OKR水印\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M6TRHmkPTCY2"
      },
      "outputs": [],
      "source": [
        "print(\"注入OKR水印...\")\n",
        "watermarked_model = inject_okr(\n",
        "    model,\n",
        "    epsilon=CONFIG[\"epsilon\"],\n",
        "    secret_key=CONFIG[\"secret_key\"]\n",
        ")\n",
        "print(\"✓ 水印注入完成\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwaSVQr3TCY2"
      },
      "source": [
        "## 6. 准备测试数据\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jSCErV1PTCY2"
      },
      "outputs": [],
      "source": [
        "# 测试提示词\n",
        "test_prompts = [\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"In a world where technology advances rapidly, artificial intelligence plays a crucial role.\",\n",
        "    \"Climate change is one of the most pressing issues facing humanity today.\",\n",
        "    \"The study of mathematics has been fundamental to scientific progress throughout history.\",\n",
        "    \"Machine learning algorithms can identify patterns in large datasets.\",\n",
        "    \"The human brain is one of the most complex structures in the known universe.\",\n",
        "    \"Renewable energy sources are becoming increasingly important for sustainable development.\",\n",
        "    \"The internet has revolutionized the way we communicate and access information.\",\n",
        "    \"Quantum computing represents a paradigm shift in computational capabilities.\",\n",
        "    \"Biodiversity conservation is essential for maintaining ecosystem stability.\"\n",
        "]\n",
        "\n",
        "# 只使用前num_samples个\n",
        "test_prompts = test_prompts[:CONFIG[\"num_samples\"]]\n",
        "print(f\"测试提示词数量: {len(test_prompts)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5sxQmlFTCY2"
      },
      "source": [
        "## 7. 生成带水印的文本\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XvWlpON5TCY2"
      },
      "outputs": [],
      "source": [
        "watermarked_texts = []\n",
        "watermarked_token_ids = []\n",
        "sample_routing_data = []\n",
        "\n",
        "decoder_start_token_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id\n",
        "\n",
        "for i, text in enumerate(tqdm(test_prompts, desc=\"生成文本\")):\n",
        "    # 清空路由数据\n",
        "    if hasattr(watermarked_model, 'clear_okr_stats'):\n",
        "        watermarked_model.clear_okr_stats()\n",
        "\n",
        "    # 编码输入\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True,\n",
        "                      max_length=CONFIG[\"max_length\"]).to(CONFIG[\"device\"])\n",
        "\n",
        "    # 生成文本\n",
        "    with torch.no_grad():\n",
        "        outputs = watermarked_model.generate(\n",
        "            **inputs,\n",
        "            max_length=CONFIG[\"max_length\"],\n",
        "            num_beams=1,\n",
        "            do_sample=False,\n",
        "            decoder_start_token_id=decoder_start_token_id,\n",
        "            pad_token_id=tokenizer.pad_token_id or tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    watermarked_texts.append(generated_text)\n",
        "    watermarked_token_ids.append(outputs[0])\n",
        "\n",
        "    # 提取路由数据\n",
        "    current_sample_routing_data = None\n",
        "    router_found = False\n",
        "    for name, module in watermarked_model.named_modules():\n",
        "        if hasattr(module, '_okr_all_selected_experts') and module._okr_all_selected_experts:\n",
        "            if not router_found:\n",
        "                all_experts = module._okr_all_selected_experts\n",
        "                if all_experts and len(all_experts) > 0:\n",
        "                    current_sample_routing_data = torch.cat(all_experts, dim=1)\n",
        "                    router_found = True\n",
        "                    break\n",
        "\n",
        "    if current_sample_routing_data is not None:\n",
        "        sample_routing_data.append(current_sample_routing_data.clone())\n",
        "    else:\n",
        "        sample_routing_data.append(None)\n",
        "\n",
        "    print(f\"样本 {i+1}: 原始='{text[:50]}...', 生成='{generated_text[:50]}...'\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3zLaQkETCY2"
      },
      "source": [
        "## 8. 检测水印\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fm8MTR1XTCY2"
      },
      "outputs": [],
      "source": [
        "detector = OKRDetector(watermarked_model, epsilon=CONFIG[\"epsilon\"])\n",
        "\n",
        "detection_results = []\n",
        "\n",
        "for i, (original_text, watermarked_text, generated_token_ids, sample_routing) in enumerate(\n",
        "    zip(test_prompts, watermarked_texts, watermarked_token_ids, sample_routing_data)\n",
        "):\n",
        "    # 准备encoder输入\n",
        "    encoder_inputs = tokenizer(original_text, return_tensors=\"pt\", padding=True, truncation=True,\n",
        "                              max_length=CONFIG[\"max_length\"]).to(CONFIG[\"device\"])\n",
        "\n",
        "    # 准备decoder输入\n",
        "    if len(generated_token_ids.shape) == 1:\n",
        "        generated_seq = generated_token_ids\n",
        "    else:\n",
        "        generated_seq = generated_token_ids[0]\n",
        "\n",
        "    if generated_seq.shape[0] > 0:\n",
        "        decoder_input_ids = torch.cat([\n",
        "            torch.tensor([[decoder_start_token_id]], device=CONFIG[\"device\"], dtype=torch.long),\n",
        "            generated_seq[:-1].unsqueeze(0) if generated_seq.shape[0] > 1 else torch.tensor([[decoder_start_token_id]], device=CONFIG[\"device\"], dtype=torch.long)\n",
        "        ], dim=1)\n",
        "    else:\n",
        "        decoder_input_ids = torch.tensor([[decoder_start_token_id]], device=CONFIG[\"device\"], dtype=torch.long)\n",
        "\n",
        "    # 设置路由数据\n",
        "    if hasattr(watermarked_model, 'clear_okr_stats'):\n",
        "        watermarked_model.clear_okr_stats()\n",
        "\n",
        "    if sample_routing is not None:\n",
        "        router_for_detection = None\n",
        "        router_found = False\n",
        "        for name, module in watermarked_model.named_modules():\n",
        "            if hasattr(module, '_okr_all_selected_experts'):\n",
        "                if not router_found:\n",
        "                    router_for_detection = module\n",
        "                    router_found = True\n",
        "                    break\n",
        "\n",
        "        if router_for_detection is not None:\n",
        "            routing_list = []\n",
        "            for j in range(sample_routing.shape[1]):\n",
        "                routing_list.append(sample_routing[:, j:j+1, :])\n",
        "            router_for_detection._okr_all_selected_experts = routing_list\n",
        "\n",
        "    # 检测\n",
        "    score, verdict = detector.detect(\n",
        "        input_ids=encoder_inputs[\"input_ids\"],\n",
        "        attention_mask=encoder_inputs.get(\"attention_mask\"),\n",
        "        decoder_input_ids=decoder_input_ids\n",
        "    )\n",
        "\n",
        "    detection_results.append({\n",
        "        \"sample_id\": i,\n",
        "        \"original_text\": original_text[:100],\n",
        "        \"watermarked_text\": watermarked_text[:100],\n",
        "        \"hit_rate\": float(score),\n",
        "        \"verdict\": verdict\n",
        "    })\n",
        "\n",
        "    print(f\"样本 {i+1}: 命中率={score:.4f}, 判定={verdict}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rC8eyrB6TCY2"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i3_pZfB5TCY2"
      },
      "outputs": [],
      "source": [
        "# 统计结果\n",
        "hit_rates = [r[\"hit_rate\"] for r in detection_results]\n",
        "avg_hit_rate = np.mean(hit_rates) if hit_rates else 0.0\n",
        "watermarked_count = sum(1 for r in detection_results if r[\"verdict\"] == \"Watermarked\")\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"实验结果汇总\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"总样本数: {len(test_prompts)}\")\n",
        "print(f\"检测为水印: {watermarked_count}\")\n",
        "print(f\"平均命中率: {avg_hit_rate:.4f}\")\n",
        "print(f\"命中率范围: [{min(hit_rates):.4f}, {max(hit_rates):.4f}]\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# 保存结果\n",
        "results = {\n",
        "    \"config\": CONFIG,\n",
        "    \"summary\": {\n",
        "        \"total_samples\": len(test_prompts),\n",
        "        \"watermarked_samples\": watermarked_count,\n",
        "        \"average_hit_rate\": float(avg_hit_rate),\n",
        "        \"hit_rate_range\": [float(min(hit_rates)), float(max(hit_rates))]\n",
        "    },\n",
        "    \"detailed_results\": detection_results\n",
        "}\n",
        "\n",
        "with open(\"okr_results.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(results, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "print(\"\\n✓ 实验结果已保存到 okr_results.json\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UhxQzaKTCY2"
      },
      "source": [
        "## 10. 可视化结果（可选）\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QiyqeIjsTCY2"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 绘制命中率分布\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(hit_rates, bins=20, edgecolor='black', alpha=0.7)\n",
        "plt.axvline(avg_hit_rate, color='r', linestyle='--', label=f'Average Hit Rate: {avg_hit_rate:.4f}')\n",
        "plt.xlabel('Hit Rate')\n",
        "plt.ylabel('Number of Samples')\n",
        "plt.title('Distribution of OKR Watermark Detection Hit Rates')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "# 打印详细结果\n",
        "print(\"\\nDetailed Results:\")\n",
        "for r in detection_results:\n",
        "    print(f\"Sample {r['sample_id']+1}: {r['verdict']} (Hit Rate: {r['hit_rate']:.4f})\")"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}