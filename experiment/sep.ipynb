{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MoE 可证鲁棒水印方案 (Google Colab 实验)\n",
    "\n",
    "本项目基于《Signal-Attack Decoupling in MoE Watermarks》及其工程方案 (`align_to_proofs.md`)，提供了一个基于 MoE Gating 机制的可证鲁棒水印方案的 Colab 实验环境。\n",
    "\n",
    "**请按照顺序执行以下单元格：**\n",
    "\n",
    "1.  **Setup Environment**：安装所有必要的 Python 依赖包。\n",
    "2.  **Write Python Files**：将项目的所有 `.py` 脚本写入 Colab 的文件系统。\n",
    "3.  **Run Experiments**：运行三种模式（`calibrate`, `embed`, `detect`）的示例命令。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Environment\n",
    "\n",
    "执行此单元格以安装所有依赖项。**安装完成后，您可能需要重启运行时（Runtime -> Restart session）。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch transformers numpy scipy scikit-learn tqdm datasets sentencepiece accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Write Python Files\n",
    "\n",
    "依次执行以下单元格，将项目的 Python 源代码写入 Colab 环境。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile moe_watermark.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModelForCausalLM\n",
    "from typing import Callable, Optional, Tuple\n",
    "\n",
    "class MoEWatermark:\n",
    "    \"\"\"\n",
    "    实现了 MoE Gating 水印的核心逻辑。\n",
    "    此类的方法将被注入 (patch) 到预训练模型的 MoE Gating 模块中。\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, K_sec: str, epsilon: float, num_experts: int, k_top: int, device: torch.device):\n",
    "        self.secret_key = K_sec\n",
    "        self.epsilon = epsilon\n",
    "        self.num_experts = num_experts\n",
    "        self.k_top = k_top\n",
    "        self.device = device\n",
    "        \n",
    "        # 理论: epsilon = Var[delta_l]\n",
    "        # 实现: 我们在 \"绿色专家\" 上施加正偏置 b, \n",
    "        # 在 k_top 个 \"红色专家\" 上施加负偏置 -b', 以保持均值接近0\n",
    "        # 这是一个简化的实现，更精确的实现需要匹配 Var\n",
    "        self.bias_strength = torch.sqrt(torch.tensor(self.epsilon, device=self.device)) * 5.0 # 简化的启发式调整\n",
    "\n",
    "    def get_context_hash(self, hidden_states: torch.Tensor) -> int:\n",
    "        \"\"\"\n",
    "        根据上下文 (hidden_states) 生成一个用于 PRNG 的种子。\n",
    "        这是一个简化的实现。\n",
    "        \"\"\"\n",
    "        # [batch_size, seq_len, dim] -> [batch_size, seq_len]\n",
    "        hashed = torch.sum(hidden_states, dim=-1).long() \n",
    "        # 使用最后一个 token 的哈希值\n",
    "        # [batch_size]\n",
    "        last_token_hash = hashed[:, -1] \n",
    "        # 合并 batch 中的哈希值\n",
    "        seed = torch.sum(last_token_hash).item()\n",
    "        return hash((self.secret_key, seed))\n",
    "\n",
    "    def get_bias_vector(self, hidden_states: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        生成偏置向量 delta_l。\n",
    "        严格遵循方案 2. 节。\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, _ = hidden_states.shape\n",
    "        context_hash = self.get_context_hash(hidden_states)\n",
    "        \n",
    "        # 使用确定性的种子\n",
    "        generator = torch.Generator(device=self.device)\n",
    "        generator.manual_seed(context_hash)\n",
    "        \n",
    "        delta_l = torch.zeros((batch_size, seq_len, self.num_experts), device=self.device)\n",
    "        \n",
    "        # 1. 选择一个 \"绿色专家\"\n",
    "        green_expert = torch.randint(0, self.num_experts, (batch_size, seq_len), generator=generator, device=self.device)\n",
    "        \n",
    "        # 2. 施加正偏置\n",
    "        delta_l.scatter_(-1, green_expert.unsqueeze(-1), self.bias_strength)\n",
    "        \n",
    "        # 3. (可选) 施加负偏置以平衡\n",
    "        # ...\n",
    "        \n",
    "        return delta_l\n",
    "\n",
    "    def watermarked_router_forward(\n",
    "        self, \n",
    "        original_forward: Callable, \n",
    "        hidden_states: torch.Tensor\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        被修补 (patched) 的前向传播函数。\n",
    "        \"\"\"\n",
    "        # 1. 原始 logits\n",
    "        # [batch_size, seq_len, num_experts]\n",
    "        l_0 = original_forward(hidden_states)\n",
    "        \n",
    "        # 2. 生成并注入偏置\n",
    "        delta_l = self.get_bias_vector(hidden_states)\n",
    "        l_1 = l_0 + delta_l\n",
    "        \n",
    "        # 3. 计算 p0 (用于检测器) 和 p1 (用于路由)\n",
    "        # 注意: p0 和 p1 都是完整的 softmax 分布\n",
    "        p_0_dist = torch.softmax(l_0, dim=-1)\n",
    "        p_1_dist = torch.softmax(l_1, dim=-1)\n",
    "        \n",
    "        # 4. Gating 路由（实际执行）\n",
    "        # 使用 l_1 (水马 logits) 进行 Top-k 选择\n",
    "        # [batch_size, seq_len, k_top]\n",
    "        top_k_scores, S_indices = torch.topk(p_1_dist, self.k_top, dim=-1)\n",
    "        \n",
    "        # 归一化 top-k 得分\n",
    "        top_k_scores = top_k_scores / (top_k_scores.sum(dim=-1, keepdim=True) + 1e-9)\n",
    "        \n",
    "        # 5. 返回检测器所需信息\n",
    "        # p0, p1 (完整分布), S_indices (k个激活索引)\n",
    "        return top_k_scores, S_indices, p_0_dist, p_1_dist, S_indices\n",
    "\n",
    "def patch_moe_model_with_watermark(\n",
    "    model: AutoModelForCausalLM, \n",
    "    K_sec: str, \n",
    "    epsilon: float\n",
    ") -> AutoModelForCausalLM:\n",
    "    \"\"\"\n",
    "    \"修补\" 一个预训练的 MoE 模型，注入水印逻辑。\n",
    "    注意：这高度依赖于模型的具体实现 (如此处的 Mixtral)。\n",
    "    \"\"\"\n",
    "    \n",
    "    # 假设模型是 Mixtral\n",
    "    if \"Mixtral\" not in model.config.model_type:\n",
    "        raise NotImplementedError(\"此修补脚本目前仅支持 Mixtral 架构。\")\n",
    "\n",
    "    num_experts = model.config.num_local_experts\n",
    "    k_top = model.config.num_experts_per_tok\n",
    "    device = model.device\n",
    "\n",
    "    watermark_injector = MoEWatermark(K_sec, epsilon, num_experts, k_top, device)\n",
    "    \n",
    "    print(f\"Patching {model.config.model_type} with MoE watermark...\")\n",
    "    \n",
    "    for layer in model.model.layers:\n",
    "        # 找到 MoE 层的 Gating network (router)\n",
    "        router = layer.block_sparse_moe.gate\n",
    "        \n",
    "        # 保存原始的 forward 方法\n",
    "        original_forward = router.forward.__get__(router)\n",
    "        \n",
    "        # 创建新的 forward 方法\n",
    "        def new_forward(hidden_states: torch.Tensor):\n",
    "            # 调用水印逻辑\n",
    "            top_k_scores, S_indices, p_0, p_1, S_obs = \\\n",
    "                watermark_injector.watermarked_router_forward(original_forward, hidden_states)\n",
    "            \n",
    "            # 将检测器信息附加到 router 对象上 (以便后续访问)\n",
    "            router._watermark_detection_data = (p_0, p_1, S_obs)\n",
    "            \n",
    "            # MoE 路由需要稀疏的 gate_logits\n",
    "            # [batch_size * seq_len, num_experts]\n",
    "            batch_size, seq_len, _ = hidden_states.shape\n",
    "            router_logits = torch.zeros(\n",
    "                (batch_size * seq_len, num_experts), \n",
    "                dtype=top_k_scores.dtype, \n",
    "                device=device\n",
    "            )\n",
    "            router_logits.scatter_(-1, S_indices.view(-1, k_top), top_k_scores.view(-1, k_top))\n",
    "            \n",
    "            return router_logits\n",
    "\n",
    "        # 应用 patch\n",
    "        router.forward = new_forward\n",
    "        \n",
    "    print(\"Patching complete.\")\n",
    "    return model\n",
    "\n",
    "def get_watermark_data_from_model(model: AutoModelForCausalLM) -> list:\n",
    "    \"\"\"\n",
    "    从模型中提取检测器所需的数据。\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    for layer in model.model.layers:\n",
    "        if hasattr(layer.block_sparse_moe.gate, '_watermark_detection_data'):\n",
    "            data.append(layer.block_sparse_moe.gate._watermark_detection_data)\n",
    "            # (可选) 用后即焚，清除数据\n",
    "            del layer.block_sparse_moe.gate._watermark_detection_data\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile calibration.py\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.linear_model import RANSACRegressor\n",
    "from transformers import AutoModelForCausalLM\n",
    "from torch.utils.data import DataLoader\n",
    "from typing import Tuple, Dict\n",
    "\n",
    "from moe_watermark import patch_moe_model_with_watermark, get_watermark_data_from_model\n",
    "from attacks import estimate_gamma_from_text, paraphrase_text_batch\n",
    "\n",
    "# 占位符：需要一个函数来计算 Chernoff 信息\n",
    "def compute_chernoff_information(p0: torch.Tensor, p1: torch.Tensor) -> float:\n",
    "    \"\"\"\n",
    "    计算 D*(p0, p1)\n",
    "    D* = -min_{lambda in [0,1]} log( sum( p0^(1-lambda) * p1^lambda ) )\n",
    "    \"\"\"\n",
    "    p0 = p0.cpu().numpy()\n",
    "    p1 = p1.cpu().numpy()\n",
    "    \n",
    "    def objective(lambda_):\n",
    "        if lambda_ < 0 or lambda_ > 1:\n",
    "            return np.inf\n",
    "        log_sum = np.log(np.sum(np.power(p0, 1 - lambda_) * np.power(p1, lambda_)) + 1e-9) # 增加稳定性\n",
    "        return -log_sum\n",
    "\n",
    "    result = minimize(objective, 0.5, bounds=[(0, 1)])\n",
    "    if result.success:\n",
    "        return result.fun\n",
    "    else:\n",
    "        # 边界情况\n",
    "        return max(objective(0), objective(1))\n",
    "\n",
    "def calibrate_Lg(model: AutoModelForCausalLM, dataloader: DataLoader, device: torch.device) -> float:\n",
    "    \"\"\"\n",
    "    标定 Lipschitz 常数 Lg (对标 Algorithm 1)\n",
    "    \"\"\"\n",
    "    print(\"Starting Lg calibration (Algorithm 1)...\")\n",
    "    model.eval()\n",
    "    ratios = []\n",
    "    \n",
    "    # 假设 dataloader 产生 embedding\n",
    "    # 在实际中，我们需要 tokenizer 和 model.get_input_embeddings()\n",
    "    # 为简化，我们假设 dataloader 直接产生 inputs_embeds\n",
    "    # NOTE: 实际dataloader 产生 'input_ids'。我们需要 'get_input_embeddings'\n",
    "    \n",
    "    embedding_layer = model.get_input_embeddings()\n",
    "    \n",
    "    for batch in tqdm(dataloader, desc=\"Calibrating Lg\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        inputs_embeds = embedding_layer(input_ids)\n",
    "        \n",
    "        # 1. 获取原始 logits l(e)\n",
    "        with torch.no_grad():\n",
    "            # Mixtral 需要 attention_mask\n",
    "            attention_mask = batch.get('attention_mask', torch.ones_like(input_ids)).to(device)\n",
    "            outputs = model(inputs_embeds=inputs_embeds, attention_mask=attention_mask, output_hidden_states=True)\n",
    "            # 我们需要 MoE 层的 *输入* hidden_states\n",
    "            # 假设我们修补第一层 MoE\n",
    "            # Mixtral router 的 forward 只有一个参数\n",
    "            hs_e = model.model.layers[0].block_sparse_moe.gate(outputs.hidden_states[0]) # MoE 层的输入\n",
    "            \n",
    "        # 2. 生成扰动 e'\n",
    "        epsilon = 0.01\n",
    "        noise = torch.randn_like(inputs_embeds) * epsilon\n",
    "        e_prime = inputs_embeds + noise\n",
    "        \n",
    "        # 3. 获取扰动 logits l(e')\n",
    "        with torch.no_grad():\n",
    "            outputs_prime = model(inputs_embeds=e_prime, attention_mask=attention_mask, output_hidden_states=True)\n",
    "            hs_e_prime = model.model.layers[0].block_sparse_moe.gate(outputs_prime.hidden_states[0])\n",
    "\n",
    "        # 4. 计算 L2 范数\n",
    "        delta_l = torch.norm(hs_e - hs_e_prime, p=2, dim=-1).view(-1)\n",
    "        delta_x = torch.norm(noise, p=2, dim=-1).view(-1)\n",
    "        \n",
    "        # 5. 计算比率\n",
    "        valid_mask = delta_x > 1e-6\n",
    "        r_i = delta_l[valid_mask] / delta_x[valid_mask]\n",
    "        ratios.extend(r_i.cpu().numpy())\n",
    "\n",
    "    if not ratios:\n",
    "        print(\"Warning: Lg calibration failed to produce ratios.\")\n",
    "        return 2.0 # 返回默认值\n",
    "\n",
    "    Lg_95 = np.percentile(ratios, 95)\n",
    "    print(f\"Lg (95th percentile) calibrated: {Lg_95:.4f}\")\n",
    "    return float(Lg_95)\n",
    "\n",
    "\n",
    "def calibrate_C(model: AutoModelForCausalLM, dataloader: DataLoader, tokenizer, device: torch.device, Lg: float) -> Tuple[float, float, float]:\n",
    "    \"\"\"\n",
    "    标定 C_prop, C_stability, C (对标 Algorithm 2)\n",
    "    \"\"\"\n",
    "    print(\"Starting C calibration (Algorithm 2)...\")\n",
    "    model.eval()\n",
    "    \n",
    "    gammas = []\n",
    "    deltas_tv = []\n",
    "    \n",
    "    # 1. 标定 C_prop\n",
    "    print(\"Calibrating C_prop...\")\n",
    "    for batch in tqdm(dataloader, desc=\"Calibrating C_prop\"):\n",
    "        inputs = batch['input_ids'].to(device)\n",
    "        text_batch = tokenizer.batch_decode(inputs, skip_special_tokens=True)\n",
    "        \n",
    "        # 2. 生成释义攻击 x'\n",
    "        text_prime_batch = paraphrase_text_batch(text_batch)\n",
    "        \n",
    "        for text, text_prime in zip(text_batch, text_prime_batch):\n",
    "            # 3. 估算 gamma\n",
    "            gamma_i = estimate_gamma_from_text(text, text_prime, tokenizer.vocab_size)\n",
    "            if gamma_i < 1e-6:\n",
    "                continue\n",
    "            \n",
    "            # 4. 计算激活分布 p(e|x) 和 p(e|x')\n",
    "            # 为简化，我们只使用第一个 MoE 层\n",
    "            def get_activation_dist(txt: str) -> torch.Tensor:\n",
    "                inputs = tokenizer(txt, return_tensors=\"pt\").to(device)\n",
    "                with torch.no_grad():\n",
    "                    model(**inputs) # 运行 forward 以填充 _watermark_detection_data\n",
    "                data = get_watermark_data_from_model(model)\n",
    "                if not data:\n",
    "                    return torch.empty(0)\n",
    "                # (p_0, p_1, S_obs)\n",
    "                # 我们需要 p_0 (原始分布)\n",
    "                # [batch, seq, K_experts]\n",
    "                p_0_dist = data[0][0] \n",
    "                # [seq, K_experts]\n",
    "                return p_0_dist.mean(dim=[0, 1]) # 取 batch 和 seq 的平均分布\n",
    "\n",
    "            p_dist = get_activation_dist(text)\n",
    "            p_prime_dist = get_activation_dist(text_prime)\n",
    "\n",
    "            if p_dist.numel() == 0 or p_prime_dist.numel() == 0:\n",
    "                continue\n",
    "                \n",
    "            # 5. 计算总变差距离 delta\n",
    "            delta_i_tv = 0.5 * torch.sum(torch.abs(p_dist - p_prime_dist))\n",
    "            \n",
    "            gammas.append(gamma_i)\n",
    "            deltas_tv.append(delta_i_tv.item())\n",
    "\n",
    "    if not gammas or not deltas_tv:\n",
    "        print(\"Warning: C_prop calibration failed (no data). Using defaults.\")\n",
    "        return 1.0, 1.0, 1.0 # 返回默认值\n",
    "\n",
    "    # 6. 稳健回归: delta ≈ C_prop * sqrt(gamma)\n",
    "    X = np.sqrt(np.array(gammas)).reshape(-1, 1)\n",
    "    y = np.array(deltas_tv)\n",
    "    \n",
    "    try:\n",
    "        ransac = RANSACRegressor().fit(X, y)\n",
    "        C_prop = ransac.estimator_.coef_[0]\n",
    "    except Exception as e:\n",
    "        print(f\"RANSAC fit failed: {e}. Using numpy.linalg.lstsq.\")\n",
    "        try:\n",
    "            C_prop = np.linalg.lstsq(X, y, rcond=None)[0][0]\n",
    "        except Exception:\n",
    "            print(\"Fallback linear fit failed. Using default C_prop=1.0\")\n",
    "            C_prop = 1.0\n",
    "            \n",
    "    print(f\"C_prop (Propagation Constant) calibrated: {C_prop:.4f}\")\n",
    "    \n",
    "    # 7. 标定 C_stability (简化)\n",
    "    # 这一步在实践中非常复杂，因为它需要计算 D*(p', q') 和 D*(p, q)\n",
    "    # 我们暂时使用一个基于 Lg 的启发式或一个默认值\n",
    "    C_stability = max(1.0, Lg / 2.0) # 启发式\n",
    "    print(f\"C_stability (Stability Constant) estimated: {C_stability:.4f}\")\n",
    "\n",
    "    # 8. 综合常数 C\n",
    "    C = C_stability * C_prop\n",
    "    print(f\"Overall System Constant C calibrated: {C:.4f}\")\n",
    "    \n",
    "    return float(C_prop), float(C_stability), float(C)\n",
    "\n",
    "def calibrate_C_star(\n",
    "    model: AutoModelForCausalLM, \n",
    "    dataloader: DataLoader, \n",
    "    tokenizer: AutoTokenizer,\n",
    "    C: float, \n",
    "    gamma_design: float, \n",
    "    device: str,\n",
    "    lambda_weight: float = 1.0,\n",
    "    delta_error: float = 0.001\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    标定最优安全系数 c* (对标 Algorithm 3)\n",
    "    \"\"\"\n",
    "    print(\"Starting c* calibration (Algorithm 3)...\")\n",
    "    \n",
    "    # 1. 标定性能成本函数 ΔA(c)\n",
    "    # 我们用 PPL (Perplexity) 作为性能指标\n",
    "    c_scan = np.linspace(C + 0.1, C * 2.5, 10)\n",
    "    delta_A_values = []\n",
    "    \n",
    "    print(\"Scanning c values for performance cost (PPL)...\")\n",
    "    \n",
    "    # 测量基线 PPL\n",
    "    base_ppl = measure_ppl(model, dataloader, device)\n",
    "    print(f\"Base PPL (no watermark): {base_ppl:.4f}\")\n",
    "    \n",
    "    # 保存原始 router \n",
    "    original_routers = {}\n",
    "    for i, layer in enumerate(model.model.layers):\n",
    "        original_routers[i] = layer.block_sparse_moe.gate.forward\n",
    "        \n",
    "    for c_val in tqdm(c_scan, desc=\"Calibrating ΔA(c)\"):\n",
    "        epsilon = c_val**2 * gamma_design\n",
    "        # K_sec 在这里是临时的，只为测量 PPL\n",
    "        temp_model = patch_moe_model_with_watermark(model, \"temp_calib_key\", epsilon)\n",
    "        \n",
    "        ppl = measure_ppl(temp_model, dataloader, device)\n",
    "        delta_A = ppl - base_ppl # 性能 *下降*，所以 ppl 越高, ΔA 越大\n",
    "        delta_A_values.append(delta_A)\n",
    "        \n",
    "        # 卸载 patch，恢复模型\n",
    "        for i, layer in enumerate(model.model.layers):\n",
    "            layer.block_sparse_moe.gate.forward = original_routers[i]\n",
    "        \n",
    "    # 拟合 ΔA(c) = a * c^p\n",
    "    # 为简化，我们使用 2 阶多项式\n",
    "    try:\n",
    "        poly_coeffs = np.polyfit(c_scan, delta_A_values, 2)\n",
    "        delta_A_func = np.poly1d(poly_coeffs)\n",
    "        print(f\"Performance cost function ΔA(c) fitted: {delta_A_func}\")\n",
    "    except np.linalg.LinAlgError:\n",
    "        print(\"Warning: PPL fitting failed. Using linear approximation.\")\n",
    "        slope = (delta_A_values[-1] - delta_A_values[0]) / (c_scan[-1] - c_scan[0] + 1e-9)\n",
    "        delta_A_func = lambda c: max(0, slope * (c - C))\n",
    "\n",
    "    # 2. 网格搜索 c*\n",
    "    # 目标函数: n*(c) + λ * ΔA(c)\n",
    "    def objective_func(c):\n",
    "        if c <= C:\n",
    "            return np.inf\n",
    "        # 样本复杂度 n*\n",
    "        n_star = np.log(1.0 / delta_error) / (gamma_design * c * (c - C) + 1e-9)\n",
    "        # 性能成本 ΔA\n",
    "        delta_A = delta_A_func(c)\n",
    "        \n",
    "        return n_star + lambda_weight * delta_A\n",
    "\n",
    "    # 3. 求解 c*\n",
    "    # 我们在 c_scan 范围内寻找最优值\n",
    "    best_c_star = c_scan[0]\n",
    "    min_obj = np.inf\n",
    "    \n",
    "    for c_val in np.linspace(C + 0.01, C * 2.5, 50): # 细网格\n",
    "        obj = objective_func(c_val)\n",
    "        if obj < min_obj:\n",
    "            min_obj = obj\n",
    "            best_c_star = c_val\n",
    "            \n",
    "    print(f\"Optimal Security Factor c* calibrated: {best_c_star:.4f}\")\n",
    "    return float(best_c_star)\n",
    "\n",
    "def measure_ppl(model: AutoModelForCausalLM, dataloader: DataLoader, device: torch.device) -> float:\n",
    "    \"\"\"\n",
    "    辅助函数：测量模型的 PPL\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    for batch in tqdm(dataloader, desc=\"Measuring PPL\", leave=False):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch.get('attention_mask', torch.ones_like(input_ids)).to(device)\n",
    "        labels = input_ids.clone()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "        \n",
    "    if num_batches == 0:\n",
    "        return 0.0\n",
    "        \n",
    "    avg_loss = total_loss / num_batches\n",
    "    ppl = np.exp(avg_loss)\n",
    "    return float(ppl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile detector.py\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from typing import Tuple\n",
    "\n",
    "from moe_watermark import get_watermark_data_from_model\n",
    "\n",
    "class LLRDetector:\n",
    "    \"\"\"\n",
    "    实现了基于 LLR 的最优检测器 (对标方案 3. 节)\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: AutoModelForCausalLM, \n",
    "        tokenizer: AutoTokenizer, \n",
    "        tau_alpha: float = 20.0 # 判决阈值，应通过 H0 实验标定\n",
    "    ):\n",
    "        self.model = model.eval()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.tau_alpha = tau_alpha\n",
    "        self.device = model.device\n",
    "\n",
    "    def compute_llr_from_data(\n",
    "        self, \n",
    "        watermark_data_list: list\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        从模型输出中计算 LLR 统计量。\n",
    "        \"\"\"\n",
    "        total_llr = 0.0\n",
    "        \n",
    "        # 遍历所有 MoE 层\n",
    "        for layer_data in watermark_data_list:\n",
    "            # (p_0, p_1, S_obs)\n",
    "            p_0_dist_batch, p_1_dist_batch, S_indices_batch = layer_data\n",
    "            \n",
    "            # [batch, seq, K_experts]\n",
    "            # [batch, seq, k_top]\n",
    "            \n",
    "            # 理论 LLR: Λ = Σ_i log( p1(S_i) / p0(S_i) )\n",
    "            # p(S_i) 是 Top-k 激活模式的概率，计算复杂。\n",
    "            \n",
    "            # 简化 LLR (如工程方案伪代码): \n",
    "            # 我们近似为 Σ_i Σ_{e in S_i} log(p1(e)/p0(e))\n",
    "            # 即只看被激活的专家的 LLR 总和\n",
    "            \n",
    "            batch_size, seq_len, k_top = S_indices_batch.shape\n",
    "            \n",
    "            # 收集 S_i 对应的 p0 和 p1 概率\n",
    "            # [batch, seq, k_top]\n",
    "            p0_S = torch.gather(p_0_dist_batch, -1, S_indices_batch)\n",
    "            p1_S = torch.gather(p_1_dist_batch, -1, S_indices_batch)\n",
    "            \n",
    "            # 防止 log(0)\n",
    "            p0_S = torch.clamp(p0_S, min=1e-9)\n",
    "            p1_S = torch.clamp(p1_S, min=1e-9)\n",
    "            \n",
    "            # 计算 LLR\n",
    "            llr_per_expert = torch.log(p1_S) - torch.log(p0_S)\n",
    "            \n",
    "            # 对 k_top 个专家求和, 然后对 batch 和 seq 求和\n",
    "            total_llr += torch.sum(llr_per_expert).item()\n",
    "            \n",
    "        return total_llr\n",
    "\n",
    "    def detect(self, text: str) -> Tuple[bool, float]:\n",
    "        \"\"\"\n",
    "        检测给定文本是否包含水印。\n",
    "        返回 (是否检测到, LLR分数)\n",
    "        \"\"\"\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512).to(self.device)\n",
    "        \n",
    "        # 运行模型 (已 patch)\n",
    "        with torch.no_grad():\n",
    "            self.model(**inputs)\n",
    "        \n",
    "        # 提取数据\n",
    "        watermark_data = get_watermark_data_from_model(self.model)\n",
    "        \n",
    "        if not watermark_data:\n",
    "            print(\"错误：未能从模型中提取到水印数据。模型是否已正确修补 (patch)？\")\n",
    "            return False, 0.0\n",
    "            \n",
    "        # 计算 LLR\n",
    "        llr_score = self.compute_llr_from_data(watermark_data)\n",
    "        \n",
    "        # 判决\n",
    "        is_detected = llr_score > self.tau_alpha\n",
    "        \n",
    "        print(f\"LLR Score: {llr_score:.4f} (Threshold: {self.tau_alpha})\")\n",
    "        \n",
    "        return is_detected, llr_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile attacks.py\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from typing import List\n",
    "\n",
    "# 全局缓存释义模型\n",
    "_paraphrase_model = None\n",
    "_paraphrase_tokenizer = None\n",
    "_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def _load_paraphrase_model():\n",
    "    \"\"\"辅助函数：加载 T5 释义模型\"\"\"\n",
    "    global _paraphrase_model, _paraphrase_tokenizer\n",
    "    if _paraphrase_model is None:\n",
    "        print(\"Loading paraphrase model (t5-base)...\")\n",
    "        model_name = \"Vamsi/T5_Paraphrase\" # 使用一个标准的 T5 释义模型\n",
    "        _paraphrase_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        _paraphrase_model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(_device)\n",
    "        _paraphrase_model.eval()\n",
    "        print(\"Paraphrase model loaded.\")\n",
    "\n",
    "def paraphrase_text_batch(text_list: List[str]) -> List[str]:\n",
    "    \"\"\"\n",
    "    对一批文本进行释义攻击。\n",
    "    \"\"\"\n",
    "    _load_paraphrase_model()\n",
    "    \n",
    "    # T5 需要一个前缀\n",
    "    inputs = _paraphrase_tokenizer(\n",
    "        [f\"paraphrase: {text}\" for text in text_list],\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    ).to(_device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = _paraphrase_model.generate(\n",
    "            **inputs,\n",
    "            max_length=512,\n",
    "            num_beams=5,\n",
    "            num_return_sequences=1,\n",
    "            early_stopping=True\n",
    "        )\n",
    "        \n",
    "    paraphrased_texts = _paraphrase_tokenizer.batch_decode(\n",
    "        outputs, \n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "    \n",
    "    # 确保输出 batch 与输入 batch 大小一致\n",
    "    num_inputs = len(text_list)\n",
    "    num_outputs_per_input = 1 # 因为 num_return_sequences=1\n",
    "    final_texts = [paraphrased_texts[i] for i in range(num_inputs)]\n",
    "        \n",
    "    return final_texts\n",
    "\n",
    "def estimate_gamma_from_text(\n",
    "    text_original: str, \n",
    "    text_attacked: str, \n",
    "    vocab_size: int\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    估算攻击强度 γ (对标文稿 7.4.1 节)\n",
    "    使用 KL 散度的上界：γ ≈ (L/N) * H(V)\n",
    "    \"\"\"\n",
    "    # 这是一个粗略的 token-level 编辑距离\n",
    "    tokens_orig = text_original.split()\n",
    "    tokens_atk = text_attacked.split()\n",
    "    \n",
    "    L = abs(len(tokens_orig) - len(tokens_atk)) # 插入/删除\n",
    "    \n",
    "    # 替换\n",
    "    for t1, t2 in zip(tokens_orig, tokens_atk):\n",
    "        if t1 != t2:\n",
    "            L += 1\n",
    "            \n",
    "    N = max(len(tokens_orig), 1)\n",
    "    \n",
    "    # H(V) = log|V| (以 nats 为单位)\n",
    "    H_V = np.log(vocab_size)\n",
    "    \n",
    "    gamma = (L / N) * H_V\n",
    "    \n",
    "    return float(gamma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile main.py\n",
    "import argparse\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import json\n",
    "import os\n",
    "\n",
    "# 确保本地文件可以被导入\n",
    "import moe_watermark\n",
    "import calibration\n",
    "import detector\n",
    "import attacks\n",
    "\n",
    "def load_model_and_tokenizer(model_name: str, device: str):\n",
    "    print(f\"Loading model and tokenizer: {model_name}...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.bfloat16, # 使用 bfloat16 节省显存\n",
    "        device_map=\"auto\", # 自动分配到 GPU\n",
    "    )\n",
    "    print(\"Model and tokenizer loaded.\")\n",
    "    return model, tokenizer\n",
    "\n",
    "def get_dataloader(dataset_name: str, split: str, tokenizer: AutoTokenizer, batch_size: int, num_samples: int):\n",
    "    print(f\"Loading dataset: {dataset_name} (split: {split})...\")\n",
    "    dataset = load_dataset(dataset_name, name=\"wikitext-103-v1\", split=split) # 示例\n",
    "    \n",
    "    # Tokenize\n",
    "    def tokenize_function(examples):\n",
    "        tokenized = tokenizer(examples[\"text\"], truncation=True, max_length=512, padding=\"max_length\")\n",
    "        return tokenized\n",
    "\n",
    "    tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "    tokenized_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "    \n",
    "    # 截取子集\n",
    "    subset_dataset = Subset(tokenized_dataset, range(min(num_samples, len(tokenized_dataset))))\n",
    "    \n",
    "    dataloader = DataLoader(subset_dataset, batch_size=batch_size)\n",
    "    print(f\"Dataset loaded with {len(subset_dataset)} samples.\")\n",
    "    return dataloader\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"MoE Provably Robust Watermark Project\")\n",
    "    parser.add_argument(\"--mode\", type=str, required=True, choices=[\"calibrate\", \"embed\", \"detect\"], help=\"操作模式\")\n",
    "    parser.add_argument(\"--model_name\", type=str, default=\"mistralai/Mixtral-8x7B-v0.1\", help=\"要使用的 MoE 模型\")\n",
    "    parser.add_argument(\"--dataset_name\", type=str, default=\"wikitext\", help=\"用于标定的数据集\")\n",
    "    parser.add_argument(\"--dataset_split\", type=str, default=\"train\", help=\"数据集分片\")\n",
    "    parser.add_argument(\"--num_calib_samples\", type=int, default=10, help=\"用于标定的样本数量 (Colab 默认值较小)\")\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=1, help=\"标定时的 batch size (Colab 默认值较小)\")\n",
    "    \n",
    "    # Embed & Detect\n",
    "    parser.add_argument(\"--prompt\", type=str, default=\"Once upon a time\", help=\"用于生成的提示\")\n",
    "    parser.add_argument(\"--text_to_check\", type=str, help=\"用于检测的文本\")\n",
    "    parser.add_argument(\"--secret_key\", type=str, default=\"DEFAULT_SECRET_KEY\", help=\"水印密钥\")\n",
    "    parser.add_argument(\"--attack\", type=str, choices=[\"none\", \"paraphrase\"], default=\"none\", help=\"在检测前施加的攻击\")\n",
    "    \n",
    "    # Watermark Params (可被 calibrate 覆盖)\n",
    "    parser.add_argument(\"--gamma_design\", type=float, default=0.03, help=\"设计的攻击强度 γ\")\n",
    "    parser.add_argument(\"--C_system\", type=float, default=1.5, help=\"系统常数 C (来自标定)\")\n",
    "    parser.add_argument(\"--c_star\", type=float, default=2.0, help=\"安全系数 c* (来自标定)\")\n",
    "    parser.add_argument(\"--tau_alpha\", type=float, default=20.0, help=\"LLR 检测阈值 τ\")\n",
    "    \n",
    "    # Colab 运行时，我们从 sys.argv 解析 (如果不是在 notebook 中)\n",
    "    # 在 Colab 中，我们使用一个默认的 args 列表\n",
    "    # args = parser.parse_args() # 在命令行中运行时使用这个\n",
    "    # 在 Colab 单元格中，我们手动创建 args (见后续单元格)\n",
    "    # 这里我们还是用 parse_args，但允许它为空，以便后续单元格可以覆盖\n",
    "    args = parser.parse_args(args=[] if 'google.colab' in str(get_ipython()) else None)\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    # ----- Colab Overrides -----\n",
    "    # 为了在 Colab 中顺利运行，我们使用小模型和少量样本\n",
    "    # **警告**: Mixtral (47B) 在 Colab 免费版上无法运行 (OOM)\n",
    "    # 请使用 T4 GPU 上的小型 MoE (如稀疏的 T5 或 DistilBERT)\n",
    "    # **为演示起见，我们将继续使用 Mixtral，但这需要 Colab Pro (A100/V100)**\n",
    "    # args.model_name = \"google/switch-base-8\" # 示例：一个更小的 MoE\n",
    "    # ---------------------------\n",
    "    \n",
    "    if args.mode == \"calibrate\":\n",
    "        # --- 标定模式 ---\n",
    "        print(\"--- Mode: Calibrate ---\")\n",
    "        # 警告：标定模式非常消耗计算资源\n",
    "        print(\"警告：标定模式计算量巨大。在 Colab (非 Pro) 上可能失败。\")\n",
    "        print(f\"使用 {args.num_calib_samples} 样本进行标定。\")\n",
    "        \n",
    "        model, tokenizer = load_model_and_tokenizer(args.model_name, device)\n",
    "        dataloader = get_dataloader(\n",
    "            args.dataset_name, \n",
    "            args.dataset_split, \n",
    "            tokenizer, \n",
    "            args.batch_size, \n",
    "            args.num_calib_samples\n",
    "        )\n",
    "        \n",
    "        # 运行标定（使用占位符/默认值）\n",
    "        try:\n",
    "            Lg = calibration.calibrate_Lg(model, dataloader, device)\n",
    "        except Exception as e:\n",
    "            print(f\"Lg 标定失败: {e}. 使用默认值 2.0\")\n",
    "            Lg = 2.0\n",
    "\n",
    "        # 为 C 标定 patch 一个临时的 key\n",
    "        patched_model_for_C = moe_watermark.patch_moe_model_with_watermark(model, \"calib_C_key\", 0.01)\n",
    "        try:\n",
    "            C_prop, C_stability, C = calibration.calibrate_C(patched_model_for_C, dataloader, tokenizer, device, Lg)\n",
    "        except Exception as e:\n",
    "            print(f\"C 标定失败: {e}. 使用默认值 1.0, 1.5, 1.5\")\n",
    "            C_prop, C_stability, C = 1.0, 1.5, 1.5\n",
    "\n",
    "        try:\n",
    "            c_star = calibration.calibrate_C_star(model, dataloader, tokenizer, C, args.gamma_design, device)\n",
    "        except Exception as e:\n",
    "            print(f\"c* 标定失败: {e}. 使用默认值 2.0\")\n",
    "            c_star = 2.0\n",
    "        \n",
    "        print(\"\\n--- Calibration Results --- (可能使用了默认值)\")\n",
    "        print(f\"Lg (95th percentile): {Lg:.4f}\")\n",
    "        print(f\"System Constant C:    {C:.4f}\")\n",
    "        print(f\"Optimal Factor c*:    {c_star:.4f}\")\n",
    "        print(\"--------------------------------------\")\n",
    "        print(\"请将这些值用于 embed 和 detect 模式\")\n",
    "\n",
    "    elif args.mode == \"embed\":\n",
    "        # --- 嵌入模式 ---\n",
    "        print(\"--- Mode: Embed ---\")\n",
    "        model, tokenizer = load_model_and_tokenizer(args.model_name, device)\n",
    "        \n",
    "        # 计算水印强度 ε\n",
    "        epsilon = args.c_star**2 * args.gamma_design\n",
    "        print(f\"Using c*={args.c_star}, γ={args.gamma_design} -> ε={epsilon:.4f}\")\n",
    "        \n",
    "        # Patch 模型\n",
    "        patched_model = moe_watermark.patch_moe_model_with_watermark(model, args.secret_key, epsilon)\n",
    "        \n",
    "        print(f\"\\nGenerating watermarked text from prompt: '{args.prompt}'...\")\n",
    "        inputs = tokenizer(args.prompt, return_tensors=\"pt\").to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = patched_model.generate(\n",
    "                **inputs, \n",
    "                max_new_tokens=100, \n",
    "                do_sample=True, # 激活采样\n",
    "                top_k=50\n",
    "            )\n",
    "            \n",
    "        watermarked_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        print(\"\\n--- Watermarked Output --- (保存此文本用于检测)\")\n",
    "        print(watermarked_text)\n",
    "        print(\"--------------------------\")\n",
    "        # 将生成的文本写入文件，以便后续单元格使用\n",
    "        with open(\"generated_text.txt\", \"w\") as f:\n",
    "            f.write(watermarked_text)\n",
    "\n",
    "    elif args.mode == \"detect\":\n",
    "        # --- 检测模式 ---\n",
    "        print(\"--- Mode: Detect ---\")\n",
    "        if not args.text_to_check:\n",
    "            # 尝试从 embed 模式生成的文件中读取\n",
    "            if os.path.exists(\"generated_text.txt\"):\n",
    "                print(\"未提供 --text_to_check, 从 generated_text.txt 中读取...\")\n",
    "                with open(\"generated_text.txt\", \"r\") as f:\n",
    "                    args.text_to_check = f.read()\n",
    "            else:\n",
    "                raise ValueError(\"--text_to_check is required for detect mode (or run embed mode first)\")\n",
    "            \n",
    "        model, tokenizer = load_model_and_tokenizer(args.model_name, device)\n",
    "        \n",
    "        # 计算水印强度 ε\n",
    "        epsilon = args.c_star**2 * args.gamma_design\n",
    "        print(f\"Loading detector with c*={args.c_star}, γ={args.gamma_design} -> ε={epsilon:.4f}\")\n",
    "\n",
    "        # Patch 模型，以便 LLR 检测器可以访问 p0 和 p1\n",
    "        patched_model = moe_watermark.patch_moe_model_with_watermark(model, args.secret_key, epsilon)\n",
    "        \n",
    "        detector = detector.LLRDetector(patched_model, tokenizer, tau_alpha=args.tau_alpha)\n",
    "        \n",
    "        text_to_check = args.text_to_check\n",
    "        \n",
    "        # 施加攻击\n",
    "        if args.attack == \"paraphrase\":\n",
    "            print(\"Applying paraphrase attack before detection...\")\n",
    "            original_text = text_to_check\n",
    "            text_to_check = attacks.paraphrase_text_batch([original_text])[0]\n",
    "            \n",
    "            gamma_est = attacks.estimate_gamma_from_text(original_text, text_to_check, tokenizer.vocab_size)\n",
    "            print(f\"Paraphrased text: '{text_to_check}'\")\n",
    "            print(f\"Estimated attack strength γ: {gamma_est:.4f}\")\n",
    "\n",
    "        print(f\"\\nDetecting watermark in text (length {len(text_to_check)})...\")\n",
    "        is_detected, llr_score = detector.detect(text_to_check)\n",
    "        \n",
    "        print(\"\\n--- Detection Result ---\")\n",
    "        if is_detected:\n",
    "            print(f\"Result: Watermark DETECTED (Score: {llr_score:.2f}) / Threshold: {args.tau_alpha}\")\n",
    "        else:\n",
    "            print(f\"Result: Watermark NOT DETECTED (Score: {llr_score:.2f}) / Threshold: {args.tau_alpha}\")\n",
    "        print(\"------------------------\")\n",
    "\n",
    "def run_main_with_args(args_list):\n",
    "    \"\"\"在 Colab 中模拟命令行参数运行 main()\"\"\"\n",
    "    import sys\n",
    "    sys.argv = ['main.py'] + args_list\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run Experiments\n",
    "\n",
    "现在您可以运行实验了。\n",
    "\n",
    "**警告：** Mixtral-8x7B (约 47B 参数) 是一个非常大的模型。在 Google Colab 的免费版 T4 GPU (16GB VRAM) 上**无法运行**，会导致显存不足 (OOM) 错误。您需要 Colab Pro 并使用 A100 (40GB) 或 V100 (16GB，也许勉强) GPU。\n",
    "\n",
    "为了演示，下面的命令被注释掉了。如果您有足够的 GPU 资源，请取消注释并运行它们。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 模式一: `calibrate` (参数标定)\n",
    "\n",
    "（**警告：** 极其消耗计算资源，在 Colab 上可能需要数小时或因超时而失败。`main.py` 已默认使用非常少的样本量 `num_calib_samples=10` 和 `batch_size=1`）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python main.py --mode calibrate --num_calib_samples 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 模式二: `embed` (水印嵌入)\n",
    "\n",
    "此模式将加载模型（需要 Colab Pro），应用水印 patch，并生成一段带水印的文本。生成的文本将保存到 `generated_text.txt` 中，供下一步检测使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python main.py --mode embed --secret_key \"colab_test_key_42\" --prompt \"The theoretical foundation of this watermark is signal-attack decoupling\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 模式三: `detect` (水印检测)\n",
    "\n",
    "此模式将加载模型，并检测 `generated_text.txt`（上一步生成）中的文本是否包含水印。\n",
    "\n",
    "**(a) 无攻击检测**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python main.py --mode detect --secret_key \"colab_test_key_42\" --attack \"none\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b) 带释义攻击 (Paraphrase) 检测**\n",
    "\n",
    "（**注意：** 这会额外加载 T5 释义模型，进一步增加显存占用）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python main.py --mode detect --secret_key \"colab_test_key_42\" --attack \"paraphrase\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}